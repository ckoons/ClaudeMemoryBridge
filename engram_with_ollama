#!/bin/bash
# Engram with Ollama - Launcher script for Ollama integration

# Get the directory where the real Engram files are located
ENGRAM_DIR="/Users/cskoons/projects/github/Engram"

# Engram now uses FAISS by default, so no NumPy compatibility check needed
# This script uses file-based memory (fallback mode) for simplicity
# For vector search with FAISS, use ./engram_with_ollama_faiss

# Set fallback mode (file-based memory)
export ENGRAM_USE_FALLBACK=1

# Default values
MODEL="llama3:8b"
PROMPT_TYPE="combined"
CLIENT_ID="ollama"
TEMPERATURE="0.7"
MAX_TOKENS="2048"
MEMORY_FUNCTIONS=true
AVAILABLE_MODELS="Claude"

# Parse command-line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --model)
      MODEL="$2"
      shift 2
      ;;
    --prompt-type)
      PROMPT_TYPE="$2"
      shift 2
      ;;
    --client-id)
      CLIENT_ID="$2"
      shift 2
      ;;
    --temperature)
      TEMPERATURE="$2"
      shift 2
      ;;
    --max-tokens)
      MAX_TOKENS="$2"
      shift 2
      ;;
    --no-memory)
      MEMORY_FUNCTIONS=false
      shift
      ;;
    --available-models)
      AVAILABLE_MODELS="$2"
      shift 2
      ;;
    --help)
      echo "Engram with Ollama - Launcher script for Ollama integration"
      echo ""
      echo "Usage: engram_with_ollama [options]"
      echo ""
      echo "Options:"
      echo "  --model MODEL             Ollama model to use (default: llama3:8b)"
      echo "  --prompt-type TYPE        System prompt type: memory, communication, combined (default: combined)"
      echo "  --client-id ID            Client ID for Engram (default: ollama)"
      echo "  --temperature TEMP        Temperature for generation (default: 0.7)"
      echo "  --max-tokens TOKENS       Maximum tokens to generate (default: 2048)"
      echo "  --no-memory               Disable memory functions"
      echo "  --available-models MODELS Space-separated list of available models (default: Claude)"
      echo "  --help                    Show this help message"
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      echo "Use --help to see available options"
      exit 1
      ;;
  esac
done

# Store the current working directory to maintain it
CURRENT_DIR="$(pwd)"

# Skip starting the Engram memory service - will use file-based approach
echo "Using file-based memory system (no server required)"

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
  echo "Error: Ollama is not running. Please start Ollama first."
  exit 1
fi

# Use absolute paths for Ollama bridge script
OLLAMA_BRIDGE="$ENGRAM_DIR/ollama_bridge.py"
echo "Using Ollama bridge: $OLLAMA_BRIDGE"

# Verify the bridge file exists
if [ ! -f "$OLLAMA_BRIDGE" ]; then
  echo "Error: Ollama bridge script not found at $OLLAMA_BRIDGE"
  echo "Please check the installation path or reinstall Engram"
  exit 1
fi

# Build command with full absolute paths for scripts but preserving current directory
CMD="python $OLLAMA_BRIDGE $MODEL --prompt-type $PROMPT_TYPE --client-id $CLIENT_ID --temperature $TEMPERATURE --max-tokens $MAX_TOKENS"

if [ "$MEMORY_FUNCTIONS" = true ]; then
  CMD="$CMD --memory-functions"
fi

for model in $AVAILABLE_MODELS; do
  CMD="$CMD --available-models $model"
done

# Start the Ollama bridge
echo "Starting Ollama bridge with Engram memory..."
echo "Model: $MODEL"
echo "Prompt type: $PROMPT_TYPE"
echo "Client ID: $CLIENT_ID"
echo "Working directory: $CURRENT_DIR"
echo ""
echo "Type 'exit' or '/quit' to exit"
echo ""

# Use fallback file-based memory (already set at top of script)

# Execute the command from the current directory
# This ensures the bridge sees the files in the user's current directory
exec $CMD