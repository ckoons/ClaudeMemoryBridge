#!/bin/bash
# Engram with Ollama - Launcher script for Ollama integration

# Get the directory where the real Engram files are located
ENGRAM_DIR="/Users/cskoons/projects/github/Engram"

# Check for NumPy compatibility issues and offer the direct FAISS solution
if [ -f "FAISS_DIRECT.md" ]; then
    # Check if NumPy version is 2.x
    NUMPY_VER=$(python -c "import numpy; print(numpy.__version__)" 2>/dev/null || echo "not-installed")
    if [[ "$NUMPY_VER" == 2.* ]]; then
        echo "⚠️  NumPy 2.x detected: $NUMPY_VER"
        echo "This version causes compatibility issues with ChromaDB."
        echo ""
        echo "Recommended solution:"
        echo "Use our direct FAISS launcher which works with NumPy 2.x:"
        echo "./engram_with_ollama_direct"
        echo ""
        echo "See FAISS_DIRECT.md for details."
        echo ""
        read -p "Continue with current script anyway? This will use file-based memory instead of vector search. (y/n) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
        echo "Continuing with ENGRAM_USE_FALLBACK=1 (file-based memory)..."
        export ENGRAM_USE_FALLBACK=1
    fi
fi

# Default values
MODEL="llama3:8b"
PROMPT_TYPE="combined"
CLIENT_ID="ollama"
TEMPERATURE="0.7"
MAX_TOKENS="2048"
MEMORY_FUNCTIONS=true
AVAILABLE_MODELS="Claude"

# Parse command-line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --model)
      MODEL="$2"
      shift 2
      ;;
    --prompt-type)
      PROMPT_TYPE="$2"
      shift 2
      ;;
    --client-id)
      CLIENT_ID="$2"
      shift 2
      ;;
    --temperature)
      TEMPERATURE="$2"
      shift 2
      ;;
    --max-tokens)
      MAX_TOKENS="$2"
      shift 2
      ;;
    --no-memory)
      MEMORY_FUNCTIONS=false
      shift
      ;;
    --available-models)
      AVAILABLE_MODELS="$2"
      shift 2
      ;;
    --help)
      echo "Engram with Ollama - Launcher script for Ollama integration"
      echo ""
      echo "Usage: engram_with_ollama [options]"
      echo ""
      echo "Options:"
      echo "  --model MODEL             Ollama model to use (default: llama3:8b)"
      echo "  --prompt-type TYPE        System prompt type: memory, communication, combined (default: combined)"
      echo "  --client-id ID            Client ID for Engram (default: ollama)"
      echo "  --temperature TEMP        Temperature for generation (default: 0.7)"
      echo "  --max-tokens TOKENS       Maximum tokens to generate (default: 2048)"
      echo "  --no-memory               Disable memory functions"
      echo "  --available-models MODELS Space-separated list of available models (default: Claude)"
      echo "  --help                    Show this help message"
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      echo "Use --help to see available options"
      exit 1
      ;;
  esac
done

# Store the current working directory to maintain it
CURRENT_DIR="$(pwd)"

# Skip starting the Engram memory service - will use file-based approach
echo "Using file-based memory system (no server required)"
# Set environment variable to force fallback memory use
export ENGRAM_USE_FALLBACK=1

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
  echo "Error: Ollama is not running. Please start Ollama first."
  exit 1
fi

# Use absolute paths for Ollama bridge script
OLLAMA_BRIDGE="$ENGRAM_DIR/ollama_bridge.py"
echo "Using Ollama bridge: $OLLAMA_BRIDGE"

# Verify the bridge file exists
if [ ! -f "$OLLAMA_BRIDGE" ]; then
  echo "Error: Ollama bridge script not found at $OLLAMA_BRIDGE"
  echo "Please check the installation path or reinstall Engram"
  exit 1
fi

# Build command with full absolute paths for scripts but preserving current directory
CMD="python $OLLAMA_BRIDGE $MODEL --prompt-type $PROMPT_TYPE --client-id $CLIENT_ID --temperature $TEMPERATURE --max-tokens $MAX_TOKENS"

if [ "$MEMORY_FUNCTIONS" = true ]; then
  CMD="$CMD --memory-functions"
fi

for model in $AVAILABLE_MODELS; do
  CMD="$CMD --available-models $model"
done

# Start the Ollama bridge
echo "Starting Ollama bridge with Engram memory..."
echo "Model: $MODEL"
echo "Prompt type: $PROMPT_TYPE"
echo "Client ID: $CLIENT_ID"
echo "Working directory: $CURRENT_DIR"
echo ""
echo "Type 'exit' or '/quit' to exit"
echo ""

# Force the use of fallback file-based memory (no vector DB)
export ENGRAM_USE_FALLBACK=1

# Execute the command from the current directory
# This ensures the bridge sees the files in the user's current directory
exec $CMD